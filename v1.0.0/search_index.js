var documenterSearchIndex = {"docs":
[{"location":"ui-guide/#UI-Guide","page":"UI Guide","title":"UI Guide","text":"","category":"section"},{"location":"ui-guide/#Home-screen","page":"UI Guide","title":"Home screen","text":"","category":"section"},{"location":"ui-guide/","page":"UI Guide","title":"UI Guide","text":"After opening the browser at the starting page (see Getting Started), you will be greeted by the following page.","category":"page"},{"location":"ui-guide/","page":"UI Guide","title":"UI Guide","text":"(Image: UI home page)","category":"page"},{"location":"ui-guide/#Loading-data","page":"UI Guide","title":"Loading data","text":"","category":"section"},{"location":"ui-guide/","page":"UI Guide","title":"UI Guide","text":"To load data, simply select one or more files from the \"Choose files\" dropdown.","category":"page"},{"location":"ui-guide/","page":"UI Guide","title":"UI Guide","text":"You will have access to all the files available in you data directory, provided that their format is supported. See also DataIngestion.is_supported.","category":"page"},{"location":"ui-guide/","page":"UI Guide","title":"UI Guide","text":"(Image: UI file selection)","category":"page"},{"location":"ui-guide/","page":"UI Guide","title":"UI Guide","text":"Upon pressing the Load button, the data is loaded in the Source table, displayed on the top right.","category":"page"},{"location":"ui-guide/","page":"UI Guide","title":"UI Guide","text":"(Image: loaded data)","category":"page"},{"location":"ui-guide/#Filtering-data","page":"UI Guide","title":"Filtering data","text":"","category":"section"},{"location":"ui-guide/","page":"UI Guide","title":"UI Guide","text":"The Filter tab allows users to filter their data. At the moment, we support checkboxes for categorical columns and min / max selectors for continuous ones.","category":"page"},{"location":"ui-guide/","page":"UI Guide","title":"UI Guide","text":"Upon clicking on Submit, the filtered data is loaded in the Selection table, displayed on the bottom right.","category":"page"},{"location":"ui-guide/","page":"UI Guide","title":"UI Guide","text":"(Image: filtered data)","category":"page"},{"location":"ui-guide/#Processing-data","page":"UI Guide","title":"Processing data","text":"","category":"section"},{"location":"ui-guide/","page":"UI Guide","title":"UI Guide","text":"Data is processed via cards, small building blocks that add new columns to the filtered data.","category":"page"},{"location":"ui-guide/","page":"UI Guide","title":"UI Guide","text":"To add a new card, click on the ï¼‹ and select the type of card you wish to add.","category":"page"},{"location":"ui-guide/","page":"UI Guide","title":"UI Guide","text":"(Image: adding a new card)","category":"page"},{"location":"ui-guide/","page":"UI Guide","title":"UI Guide","text":"You can add and compile as many cards as you wish.","category":"page"},{"location":"ui-guide/","page":"UI Guide","title":"UI Guide","text":"Upon clicking on Submit, the additional columns are added to the Selection table.","category":"page"},{"location":"ui-guide/","page":"UI Guide","title":"UI Guide","text":"(Image: processed data)","category":"page"},{"location":"lib/DataIngestion/#DataIngestion","page":"DataIngestion API","title":"DataIngestion","text":"","category":"section"},{"location":"lib/DataIngestion/#Ingestion-interface","page":"DataIngestion API","title":"Ingestion interface","text":"","category":"section"},{"location":"lib/DataIngestion/#DataIngestion.is_supported","page":"DataIngestion API","title":"DataIngestion.is_supported","text":"is_supported(file::AbstractString)\n\nDenote whether a file is of one of the available formats:\n\njson,\ntsv,\ntxt,\ncsv,\nparquet.\n\n\n\n\n\n","category":"function"},{"location":"lib/DataIngestion/#DataIngestion.acceptable_paths","page":"DataIngestion API","title":"DataIngestion.acceptable_paths","text":"acceptable_paths()\n\nList of relative paths corresponding to supported files within DATA_DIR[].\n\n\n\n\n\n","category":"function"},{"location":"lib/DataIngestion/#DataIngestion.load_files","page":"DataIngestion API","title":"DataIngestion.load_files","text":"load_files(\n    repository::Repository,\n    files::AbstractVector{<:AbstractString};\n    format::AbstractString,\n    schema = nothing,\n    union_by_name = true, kwargs...)\n)\n\nLoad files into a table called TABLE_NAMES.source inside repository.db within the schema schema (defaults to main schema).\n\nThe format is inferred or can be passed explicitly.\n\nThe following formats are supported:\n\njson,\ntsv,\ntxt,\ncsv,\nparquet.\n\nunion_by_name and the remaining keyword arguments are forwarded to the reader for the given format.\n\n\n\n\n\n","category":"function"},{"location":"lib/DataIngestion/#Internal","page":"DataIngestion API","title":"Internal","text":"","category":"section"},{"location":"lib/DataIngestion/#DataIngestion.parse_paths","page":"DataIngestion API","title":"DataIngestion.parse_paths","text":"parse_paths(d::AbstractDict)::Vector{String}\n\nGenerate a list of file paths based on a configuration dictionary. The file paths are interpreted as relative to DataIngestion.DATA_DIR[].\n\n\n\n\n\n","category":"function"},{"location":"lib/DataIngestion/#Metadata-for-filter-generation","page":"DataIngestion API","title":"Metadata for filter generation","text":"","category":"section"},{"location":"lib/DataIngestion/#DataIngestion.summarize","page":"DataIngestion API","title":"DataIngestion.summarize","text":"summarize(repository::Repository, tbl::AbstractString; schema = nothing)\n\nCompute summaries of variables in table tbl within the database repository.db. The summary of a variable depends on its type, according to the following rules.\n\nCategorical variable => list of unique types.\nContinuous variable => extrema.\n\n\n\n\n\n","category":"function"},{"location":"lib/DataIngestion/#Filtering-interface","page":"DataIngestion API","title":"Filtering interface","text":"","category":"section"},{"location":"lib/DataIngestion/#DataIngestion.Filter","page":"DataIngestion API","title":"DataIngestion.Filter","text":"abstract type Filter end\n\nAbstract supertype to encompass all possible filters.\n\nCurrent implementations:\n\nIntervalFilter,\nListFilter.\n\n\n\n\n\n","category":"type"},{"location":"lib/DataIngestion/#DataIngestion.Filter-Tuple{AbstractDict}","page":"DataIngestion API","title":"DataIngestion.Filter","text":"Filter(d::AbstractDict)\n\nGenerate a Filter based on a configuration dictionary.\n\n\n\n\n\n","category":"method"},{"location":"lib/DataIngestion/#DataIngestion.select","page":"DataIngestion API","title":"DataIngestion.select","text":"select(repository::Repository, filters::AbstractVector; schema = nothing)\n\nCreate a table with name TABLE_NAMES.selection within the database repository.db, where repository is a Repository. The table TABLE_NAMES.selection is filled with rows from the table TABLE_NAMES.source that are kept by the filters in filters.\n\nEach filter should be an instance of Filter.\n\n\n\n\n\n","category":"function"},{"location":"lib/DataIngestion/#Filters","page":"DataIngestion API","title":"Filters","text":"","category":"section"},{"location":"lib/DataIngestion/#DataIngestion.IntervalFilter","page":"DataIngestion API","title":"DataIngestion.IntervalFilter","text":"struct IntervalFilter{T} <: Filter\n    colname::String\n    interval::ClosedInterval{T}\nend\n\nObject to retain only those rows for which the variable colname lies inside the interval.\n\n\n\n\n\n","category":"type"},{"location":"lib/DataIngestion/#DataIngestion.ListFilter","page":"DataIngestion API","title":"DataIngestion.ListFilter","text":"struct ListFilter{T} <: Filter\n    colname::String\n    list::Vector{T}\nend\n\nObject to retain only those rows for which the variable colname belongs to a list of options.\n\n\n\n\n\n","category":"type"},{"location":"getting-started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting-started/","page":"Getting Started","title":"Getting Started","text":"DashiBoard is still in development, thus installing requires a few passages.","category":"page"},{"location":"getting-started/#Installation-dependencies","page":"Getting Started","title":"Installation dependencies","text":"","category":"section"},{"location":"getting-started/","page":"Getting Started","title":"Getting Started","text":"Julia programming language (minimum version 1.11, installable via juliaup).\nJavaScript package manager pnpm.","category":"page"},{"location":"getting-started/#Launching-the-server","page":"Getting Started","title":"Launching the server","text":"","category":"section"},{"location":"getting-started/","page":"Getting Started","title":"Getting Started","text":"Open a terminal at the top-level of the repository.","category":"page"},{"location":"getting-started/","page":"Getting Started","title":"Getting Started","text":"Install all required dependencies with the following command:","category":"page"},{"location":"getting-started/","page":"Getting Started","title":"Getting Started","text":"julia --project=DashiBoard -e 'using Pkg; Pkg.instantiate()'","category":"page"},{"location":"getting-started/","page":"Getting Started","title":"Getting Started","text":"Then, launch the server with the following command:","category":"page"},{"location":"getting-started/","page":"Getting Started","title":"Getting Started","text":"julia --project=DashiBoard bin/launch.jl path/to/data","category":"page"},{"location":"getting-started/","page":"Getting Started","title":"Getting Started","text":"where path/to/data represents the data folder you wish to make accessible to DashiBoard.","category":"page"},{"location":"getting-started/#Launching-the-frontend","page":"Getting Started","title":"Launching the frontend","text":"","category":"section"},{"location":"getting-started/","page":"Getting Started","title":"Getting Started","text":"Open a terminal in the frontend folder.","category":"page"},{"location":"getting-started/","page":"Getting Started","title":"Getting Started","text":"Install all required dependencies with the following command:","category":"page"},{"location":"getting-started/","page":"Getting Started","title":"Getting Started","text":"pnpm install","category":"page"},{"location":"getting-started/","page":"Getting Started","title":"Getting Started","text":"Then, launch the frontend with the following command:","category":"page"},{"location":"getting-started/","page":"Getting Started","title":"Getting Started","text":"pnpm run start","category":"page"},{"location":"getting-started/","page":"Getting Started","title":"Getting Started","text":"To interact with the UI, open your browser and navigate to the page http://localhost:3000.","category":"page"},{"location":"lib/Pipelines/#Pipelines","page":"Pipelines API","title":"Pipelines","text":"","category":"section"},{"location":"lib/Pipelines/","page":"Pipelines API","title":"Pipelines API","text":"Pipelines is a library designed to generate and evaluate data analysis pipelines.","category":"page"},{"location":"lib/Pipelines/#Transformation-interface","page":"Pipelines API","title":"Transformation interface","text":"","category":"section"},{"location":"lib/Pipelines/#Pipelines.Card","page":"Pipelines API","title":"Pipelines.Card","text":"abstract type Card end\n\nAbstract supertype to encompass all possible cards.\n\nCurrent implementations:\n\nSplitCard,\nRescaleCard,\nClusterCard,\nDimensionalityReductionCard,\nGLMCard,\nInterpCard,\nGaussianEncodingCard,\nStreamlinerCard,\nWildCard.\n\n\n\n\n\n","category":"type"},{"location":"lib/Pipelines/#Pipelines.train","page":"Pipelines API","title":"Pipelines.train","text":"train(repository::Repository, card::Card, source; schema = nothing)::CardState\n\nReturn a trained model for a given card on a table table in the database repository.db.\n\n\n\n\n\n","category":"function"},{"location":"lib/Pipelines/#Pipelines.evaluate","page":"Pipelines API","title":"Pipelines.evaluate","text":"evaluate(\n    repository::Repository,\n    card::Card,\n    state::CardState,\n    (source, destination)::Pair,\n    id::AbstractString;\n    schema = nothing\n)\n\nReplace table destination in the database repository.db with the outcome of executing the card on the table source. The new table destination will have an additional column id, to be joined with the row number of the original table.\n\nHere, state represents the result of train(repository, card, source; schema). See also train.\n\n\n\n\n\n","category":"function"},{"location":"lib/Pipelines/#Pipelines.get_inputs","page":"Pipelines API","title":"Pipelines.get_inputs","text":"get_inputs(c::Card; invert::Bool = false, train::Bool = !invert)::Vector{String}\n\nReturn the list of inputs for a given card.\n\n\n\n\n\n","category":"function"},{"location":"lib/Pipelines/#Pipelines.get_outputs","page":"Pipelines API","title":"Pipelines.get_outputs","text":"get_outputs(c::Card; invert::Bool = false)::Vector{String}\n\nReturn the list of outputs for a given card.\n\n\n\n\n\n","category":"function"},{"location":"lib/Pipelines/#Pipelines.invertible","page":"Pipelines API","title":"Pipelines.invertible","text":"invertible(c::Card)::Bool\n\nReturn true for invertible cards, false otherwise.\n\n\n\n\n\n","category":"function"},{"location":"lib/Pipelines/#Pipeline-computation","page":"Pipelines API","title":"Pipeline computation","text":"","category":"section"},{"location":"lib/Pipelines/#Pipelines.Node","page":"Pipelines API","title":"Pipelines.Node","text":"Node(\n    card::Card,\n    state = CardState();\n    update::Bool = true\n)\n\nGenerate a Node object from a Card.\n\n\n\n\n\n","category":"type"},{"location":"lib/Pipelines/#Pipelines.train!","page":"Pipelines API","title":"Pipelines.train!","text":"train!(\n    repository::Repository,\n    node::Node,\n    table::AbstractString;\n    schema = nothing\n)\n\nTrain node on table table in repository. The field state of node is modified.\n\nSee also evaljoin, train_evaljoin!.\n\n\n\n\n\n","category":"function"},{"location":"lib/Pipelines/#Pipelines.evaljoin","page":"Pipelines API","title":"Pipelines.evaljoin","text":"evaljoin(\n    repository::Repository,\n    nodes::AbstractVector,\n    table::AbstractString,\n    [keep_vars];\n    schema = nothing\n)\n\nevaljoin(\n    repository::Repository,\n    node::Node,\n    (source, destination)::Pair,\n    [keep_vars];\n    schema = nothing\n)\n\nReplace table in the database repository.db with the outcome of executing all the transformations in nodes, without training the nodes. The resulting outputs of the pipeline are joined with the original columns keep_vars (defaults to keeping all columns).\n\nIf only a node is provided, then one should pass both source and destination tables.\n\nSee also train!, train_evaljoin!.\n\nReturn pipeline graph and metadata.\n\n\n\n\n\n","category":"function"},{"location":"lib/Pipelines/#Pipelines.train_evaljoin!","page":"Pipelines API","title":"Pipelines.train_evaljoin!","text":"train_evaljoin!(\n    repository::Repository,\n    nodes::AbstractVector,\n    table::AbstractString,\n    [keep_vars];\n    schema = nothing\n)\n\ntrain_evaljoin!(\n    repository::Repository,\n    node::Node,\n    (source, destination)::Pair,\n    [keep_vars];\n    schema = nothing\n)\n\nReplace table in the database repository.db with the outcome of executing all the transformations in nodes, after having trained the nodes. The resulting outputs of the pipeline are joined with the original columns keep_vars (defaults to keeping all columns).\n\nIf only a node is provided, then one should pass both source and destination tables.\n\nSee also train!, evaljoin.\n\nReturn pipeline graph and metadata.\n\n\n\n\n\n","category":"function"},{"location":"lib/Pipelines/#Pipeline-reports","page":"Pipelines API","title":"Pipeline reports","text":"","category":"section"},{"location":"lib/Pipelines/#Pipelines.report","page":"Pipelines API","title":"Pipelines.report","text":"report(repository::Repository, nodes::AbstractVector)\n\nCreate default reports for all nodes referring to a given repository. Each node must be of type Node.\n\n\n\n\n\nreport(::Repository, ::Card, ::CardState)\n\nOverload this method (replacing Card with a specific card type) to implement a default report for a given card type.\n\n\n\n\n\n","category":"function"},{"location":"lib/Pipelines/#Pipeline-visualizations","page":"Pipelines API","title":"Pipeline visualizations","text":"","category":"section"},{"location":"lib/Pipelines/#Pipelines.visualize","page":"Pipelines API","title":"Pipelines.visualize","text":"visualize(repository::Repository, nodes::AbstractVector)\n\nCreate default visualizations for all nodes referring to a given repository. Each node must be of type Node.\n\n\n\n\n\nvisualize(::Repository, ::Card, ::CardState)\n\nOverload this method (replacing Card with a specific card type) to implement a default visualization for a given card type.\n\n\n\n\n\n","category":"function"},{"location":"lib/Pipelines/#Cards","page":"Pipelines API","title":"Cards","text":"","category":"section"},{"location":"lib/Pipelines/#Pipelines.SplitCard","page":"Pipelines API","title":"Pipelines.SplitCard","text":"struct SplitCard <: Card\n    type::String\n    label::String\n    method::String\n    splitter::SplittingMethod\n    order_by::Vector{String}\n    by::Vector{String}\n    output::String\nend\n\nCard to split the data into two groups according to a given function splitter.\n\nCurrently supported methods are\n\ntiles (requires tiles argument, e.g., tiles = [1, 1, 2, 1, 1, 2]),\npercentile (requires percentile argument, e.g. percentile = 0.9).\n\n\n\n\n\n","category":"type"},{"location":"lib/Pipelines/#Pipelines.RescaleCard","page":"Pipelines API","title":"Pipelines.RescaleCard","text":"struct RescaleCard <: Card\n    type::String\n    label::String\n    by::Vector{String}\n    inputs::Vector{String}\n    targets::Vector{String}\n    partition::Union{String, Nothing}\n    suffix::String\n    target_suffix::Union{String, Nothing}\nend\n\nCard to rescale one or more columns according to a given rescaler. The supported methods are\n\nzscore,\nmaxabs,\nminmax,\nlog,\nlogistic.\n\nThe resulting rescaled variable is added to the table under the name \"$(originalname)_$(suffix)\".\n\n\n\n\n\n","category":"type"},{"location":"lib/Pipelines/#Pipelines.ClusterCard","page":"Pipelines API","title":"Pipelines.ClusterCard","text":"struct ClusterCard <: Card\n    type::String\n    label::String\n    method::String\n    clusterer::ClusteringMethod\n    inputs::Vector{String}\n    weights::Union{String, Nothing}\n    partition::Union{String, Nothing}\n    output::String\nend\n\nCluster inputs based on clusterer. Save resulting column as output.\n\n\n\n\n\n","category":"type"},{"location":"lib/Pipelines/#Pipelines.DimensionalityReductionCard","page":"Pipelines API","title":"Pipelines.DimensionalityReductionCard","text":"struct DimensionalityReductionCard <: Card\n    type::String\n    label::String\n    method::String\n    projector::ProjectionMethod\n    inputs::Vector{String}\n    partition::Union{String, Nothing}\n    n_components::Int\n    output::String\nend\n\nProject inputs based on projector. Save resulting column as output.\n\n\n\n\n\n","category":"type"},{"location":"lib/Pipelines/#Pipelines.GLMCard","page":"Pipelines API","title":"Pipelines.GLMCard","text":"struct GLMCard <: Card\n  type::String\n  label::String\n  distribution_name::String\n  distribution::Distribution\n  link_name::Union{String, Nothing}\n  link::Link\n  inputs::Vector{Any}\n  target::String\n  formula::FormulaTerm\n  weights::Union{String, Nothing}\n  partition::Union{String, Nothing}\n  suffix::String\nend\n\nRun a Generalized Linear Model (GLM) based on formula.\n\n\n\n\n\n","category":"type"},{"location":"lib/Pipelines/#Pipelines.InterpCard","page":"Pipelines API","title":"Pipelines.InterpCard","text":"struct InterpCard <: Card\n    type::String\n    label::String\n    method::String\n    interpolator::InterpolationMethod\n    input::String\n    targets::Vector{String}\n    partition::Union{String, Nothing} = nothing\n    suffix::String = \"hat\"\nend\n\nInterpolate targets based on input.\n\n\n\n\n\n","category":"type"},{"location":"lib/Pipelines/#Pipelines.GaussianEncodingCard","page":"Pipelines API","title":"Pipelines.GaussianEncodingCard","text":"struct GaussianEncodingCard <: Card\n\nDefines a card for applying Gaussian transformations to a specified column.\n\nFields:\n\ntype::String: Card type, i.e., \"gaussian_encoding\".\nlabel::String: Label to represent the card in a UI.\nmethod::String: Name of the processing method (see below).\ntemporal_preprocessor::TemporalProcessingMethod: Tranformation to process a given column (see below).\ninput::String: Name of the column to transform.\nn_components::Int: Number of Gaussian curves to generate.\nlambda::Float64: Coefficient for scaling the standard deviation.\nsuffix::String: Suffix added to the output column names.\n\nNotes:\n\nThe method field determines the preprocessing applied to the column.\nNo automatic selection based on column type. The user must ensure compatibility:\n\"identity\": Assumes the column is numeric.\n\"dayofyear\": Assumes the column is a date or timestamp.\n\"hourofday\": Assumes the column is a time or timestamp.\n\nMethods:\n\nDefined in the TEMPORAL_PREPROCESSING_METHODS dictionary:\n\"identity\": No transformation.\n\"dayofyear\": Applies the SQL dayofyear function.\n\"hourofday\": Applies the SQL hour function.\n\"minuteofhour\": Computes the minute within the hour.\n\"minuteofday\": Computes the minute within the day.\n\nTrain:\n\nReturns: SimpleTable (Dict{String, AbstractVector}) with Gaussian parameters:\nÏƒ: Standard deviation for Gaussian transformations.\nd: Normalization value.\nÎ¼_1, Î¼_2, ..., Î¼_n: Gaussian means.\n\nEvaluate:\n\nSteps:\nPreprocesses the column using the specified method.\nTemporarily registers the Gaussian parameters (params_tbl) using with_table.\nJoins the source table with the params table via a CROSS JOIN.\nComputes Gaussian-transformed columns.\nSelects only the required columns (original and transformed).\nReplaces the target table with the final results.\n\n\n\n\n\n","category":"type"},{"location":"lib/Pipelines/#Pipelines.StreamlinerCard","page":"Pipelines API","title":"Pipelines.StreamlinerCard","text":"struct StreamlinerCard <: Card\n    type::String\n    label::String\n    model_name::String\n    model::Model\n    training_name::String\n    training::Training\n    order_by::Vector{String}\n    inputs::Vector{String}\n    targets::Vector{String}\n    partition::Union{String, Nothing} = nothing\n    suffix::String = \"hat\"\nend\n\nRun a Streamliner model, predicting targets from inputs.\n\n\n\n\n\n","category":"type"},{"location":"lib/Pipelines/#Pipelines.WildCard","page":"Pipelines API","title":"Pipelines.WildCard","text":"struct WildCard{train, evaluate} <: Card\n    type::String\n    label::String\n    order_by::Vector{String}\n    inputs::Vector{String}\n    targets::Vector{String}\n    weights::Union{String, Nothing}\n    partition::Union{String, Nothing}\n    outputs::Vector{String}\nend\n\nCustom card that uses arbitrary training and evaluations functions.\n\n\n\n\n\n","category":"type"},{"location":"lib/Pipelines/#Card-registration","page":"Pipelines API","title":"Card registration","text":"","category":"section"},{"location":"lib/Pipelines/#Pipelines.register_card","page":"Pipelines API","title":"Pipelines.register_card","text":"register_card(config::CardConfig)\n\nSet a given card configuration as globally available.\n\nSee also CardConfig.\n\n\n\n\n\n","category":"function"},{"location":"lib/Pipelines/#Pipelines.CardConfig","page":"Pipelines API","title":"Pipelines.CardConfig","text":"@kwdef struct CardConfig{T <: Card}\n    key::String\n    label::String\n    needs_targets::Bool\n    needs_order::Bool\n    allows_weights::Bool\n    allows_partition::Bool\n    widget_configs::StringDict = StringDict()\n    methods::StringDict = StringDict()\nend\n\nConfiguration used to register a card.\n\n\n\n\n\n","category":"type"},{"location":"lib/StreamlinerCore/#StreamlinerCore","page":"StreamlinerCore API","title":"StreamlinerCore","text":"","category":"section"},{"location":"lib/StreamlinerCore/","page":"StreamlinerCore API","title":"StreamlinerCore API","text":"StreamlinerCore is a julia library to generate, train and evaluate models defined via some configuration files.","category":"page"},{"location":"lib/StreamlinerCore/#Data-interface","page":"StreamlinerCore API","title":"Data interface","text":"","category":"section"},{"location":"lib/StreamlinerCore/#StreamlinerCore.AbstractData","page":"StreamlinerCore API","title":"StreamlinerCore.AbstractData","text":"AbstactData{N}\n\nAbstract type representing streamers of N datasets. In general, StreamlinerCore will use N = 1 to validate and evaluate trained models and N = 2 to train models via a training and a validation datasets.\n\nSubtypes of AbstractData are meant to implement the following methods:\n\nstream,\nget_templates,\nget_metadata,\nget_nsamples.\n\n\n\n\n\n","category":"type"},{"location":"lib/StreamlinerCore/#StreamlinerCore.stream","page":"StreamlinerCore API","title":"StreamlinerCore.stream","text":"stream(f, data::AbstractData, partition::Integer, streaming::Streaming)\n\nStream partition of data by batches of batchsize on a given device. Return the result of applying f on the resulting batch iterator. Shuffling is optional and controlled by shuffle (boolean) and by the random number generator rng.\n\nThe options device, batchsize, shuffle, rng are passed via the configuration struct streaming::Streaming. See also Streaming.\n\n\n\n\n\n","category":"function"},{"location":"lib/StreamlinerCore/#StreamlinerCore.ingest","page":"StreamlinerCore API","title":"StreamlinerCore.ingest","text":"ingest(data::AbstractData{1}, eval_stream, select)\n\nIngest output of evaluate into a suitable database, tensor or iterator. select determines which fields of the model output to keep.\n\n\n\n\n\n","category":"function"},{"location":"lib/StreamlinerCore/#StreamlinerCore.get_templates","page":"StreamlinerCore API","title":"StreamlinerCore.get_templates","text":"get_templates(data::AbstractData)\n\nExtract templates for data. Templates encode type and size of the arrays that data will stream. See also Template\n\n\n\n\n\n","category":"function"},{"location":"lib/StreamlinerCore/#StreamlinerCore.get_metadata","page":"StreamlinerCore API","title":"StreamlinerCore.get_metadata","text":"get_metadata(x)::Dict{String, Any}\n\nExtract metadata for x. metadata should be a dictionary of information that identifies x univoquely. get_metadata has methods for AbstractData, Model, and Training.\n\n\n\n\n\n","category":"function"},{"location":"lib/StreamlinerCore/#StreamlinerCore.get_nsamples","page":"StreamlinerCore API","title":"StreamlinerCore.get_nsamples","text":"get_nsamples(data::AbstractData{N})::NTuple{N, Int} where {N}\n\nReturn number of samples for data.\n\n\n\n\n\n","category":"function"},{"location":"lib/StreamlinerCore/#StreamlinerCore.Template","page":"StreamlinerCore API","title":"StreamlinerCore.Template","text":"Template(::Type{T}, size::NTuple{N, Int}) where {T, N}\n\nCreate an object of type Template. It represents arrays with eltype T and size size. Note that size does not include the minibatch dimension.\n\n\n\n\n\n","category":"type"},{"location":"lib/StreamlinerCore/#Parser","page":"StreamlinerCore API","title":"Parser","text":"","category":"section"},{"location":"lib/StreamlinerCore/#StreamlinerCore.Parser","page":"StreamlinerCore API","title":"StreamlinerCore.Parser","text":"Parser(;\n    model, layers, sigmas, aggregators, metrics, regularizations,\n    optimizers, schedules, stoppers, devices\n)\n\nCollection of dictionaries to performance the necessary conversion from the user-specified configuration file or dictionary to julia objects.\n\nFor most usecases, one should define a default parser\n\nparser = default_parser()\n\nand pass it to Model and Training upon construction.\n\nA parser object is also required to use interface functions that read from the MongoDB:\n\nfinetune,\nloadmodel,\nvalidate,\nevaluate.\n\nSee default_parser for more advanced uses.\n\n\n\n\n\n","category":"type"},{"location":"lib/StreamlinerCore/#StreamlinerCore.default_parser","page":"StreamlinerCore API","title":"StreamlinerCore.default_parser","text":"default_parser(; plugins::AbstractVector{Parser}=Parser[])\n\nReturn a parser::Parser object that includes StreamlinerCore defaults together with optional plugins.\n\n\n\n\n\n","category":"function"},{"location":"lib/StreamlinerCore/#Parsed-objects","page":"StreamlinerCore API","title":"Parsed objects","text":"","category":"section"},{"location":"lib/StreamlinerCore/#StreamlinerCore.Model","page":"StreamlinerCore API","title":"StreamlinerCore.Model","text":"Model(parser::Parser, metadata::AbstractDict)\n\nModel(parser::Parser, path::AbstractString, [vars::AbstractDict])\n\nCreate a Model object from a configuration dictionary metadata or, alternatively, from a configuration dictionary stored at path in TOML format. The optional argument vars is a dictionary of variables the can be used to fill the template given in path.\n\nThe parser::Parser handles conversion from configuration variables to julia objects.\n\nGiven a model::Model object, use model(data) where data::AbstractData to instantiate the corresponding neural network or machine.\n\n\n\n\n\n","category":"type"},{"location":"lib/StreamlinerCore/#StreamlinerCore.Training","page":"StreamlinerCore API","title":"StreamlinerCore.Training","text":"Training(parser::Parser, metadata::AbstractDict)\n\nTraining(parser::Parser, path::AbstractString, [vars::AbstractDict])\n\nCreate a Training object from a configuration dictionary metadata or, alternatively, from a configuration dictionary stored at path in TOML format. The optional argument vars is a dictionary of variables the can be used to fill the template given in path.\n\nThe parser::Parser handles conversion from configuration variables to julia objects.\n\n\n\n\n\n","category":"type"},{"location":"lib/StreamlinerCore/#StreamlinerCore.Streaming","page":"StreamlinerCore API","title":"StreamlinerCore.Streaming","text":"Streaming(parser::Parser, metadata::AbstractDict)\n\nStreaming(parser::Parser, path::AbstractString, [vars::AbstractDict])\n\nCreate a Streaming object from a configuration dictionary metadata or, alternatively, from a configuration dictionary stored at path in TOML format. The optional argument vars is a dictionary of variables the can be used to fill the template given in path.\n\nThe parser::Parser handles conversion from configuration variables to julia objects.\n\n\n\n\n\n","category":"type"},{"location":"lib/StreamlinerCore/#Training-and-evaluation","page":"StreamlinerCore API","title":"Training and evaluation","text":"","category":"section"},{"location":"lib/StreamlinerCore/#StreamlinerCore.Result","page":"StreamlinerCore API","title":"StreamlinerCore.Result","text":"@kwdef struct Result{P}\n    iteration::Int\n    iterations::Int\n    stats::NTuple{P, Vector{Float64}}\n    trained::Bool\n    resumed::Maybe{Bool} = nothing\n    successful::Maybe{Bool} = nothing\nend\n\nStructure to encode the result of train, finetune, or validate. Stores configuration of model, metrics, and information on the location of the model weights.\n\n\n\n\n\n","category":"type"},{"location":"lib/StreamlinerCore/#StreamlinerCore.has_weights","page":"StreamlinerCore API","title":"StreamlinerCore.has_weights","text":"has_weights(result::Result)\n\nReturn true if result is a successful training result, false otherwise.\n\n\n\n\n\n","category":"function"},{"location":"lib/StreamlinerCore/#StreamlinerCore.train","page":"StreamlinerCore API","title":"StreamlinerCore.train","text":"train(\n    dir::AbstractString,\n    model::Model, data::AbstractData{2}, training::Training;\n    callback = default_callback\n)\n\nTrain model using the training configuration on data. Save the resulting weights in dir.\n\nAfter every epoch, callback(m, trace).\n\nThe arguments of callback work as follows.\n\nm is the instantiated neural network or machine,\ntrace is an object encoding additional information, i.e.,\nstats (average of metrics computed so far),\nmetrics (functions used to compute stats), and\niteration.\n\n\n\n\n\n","category":"function"},{"location":"lib/StreamlinerCore/#StreamlinerCore.finetune","page":"StreamlinerCore API","title":"StreamlinerCore.finetune","text":"finetune(\n    (src, dst)::Pair,\n    model::Model, data::AbstractData{2}, training::Training;\n    init::Maybe{Result} = nothing, callback = default_callback\n)\n\nLoad model encoded in model from src and retrain it using the training configuration on data. Save the resulting weights in dst.\n\nUse init = result::Result to restart training where it left off. The callback keyword argument works as in train.\n\n\n\n\n\n","category":"function"},{"location":"lib/StreamlinerCore/#StreamlinerCore.loadmodel","page":"StreamlinerCore API","title":"StreamlinerCore.loadmodel","text":"loadmodel(model::Model, data::AbstractData, device)\n\nLoad model encoded in model on the device. The object data is required as the model can only be initialized once the data dimensions are known.\n\n\n\n\n\nloadmodel(dirname::AbstractString, model::Model, data::AbstractData, device)\n\nLoad model encoded in result on the device. The object data is required as the model can only be initialized once the data dimensions are known.\n\n\n\n\n\n","category":"function"},{"location":"lib/StreamlinerCore/#StreamlinerCore.validate","page":"StreamlinerCore API","title":"StreamlinerCore.validate","text":"validate(\n    dir::AbstractString,\n    model::Model,\n    data::AbstractData{1},\n    streaming::Streaming\n)\n\nLoad model with weights saved in dir and validate it on data using streaming settings streaming.\n\n\n\n\n\n","category":"function"},{"location":"lib/StreamlinerCore/#StreamlinerCore.evaluate","page":"StreamlinerCore API","title":"StreamlinerCore.evaluate","text":"evaluate(\n        device_m, data::AbstractData{1}, streaming::Streaming,\n        select::SymbolTuple = (:prediction,)\n    )\n\nEvaluate model device_m on data using streaming settings streaming.\n\n\n\n\n\nevaluate(\n    dirname::AbstractString,\n    model::Model, data::AbstractData{1}, streaming::Streaming,\n    select::SymbolTuple = (:prediction,)\n)\n\nLoad model with weights saved in dirname and evaluate it on data using streaming settings streaming.\n\n\n\n\n\n","category":"function"},{"location":"lib/StreamlinerCore/#StreamlinerCore.summarize","page":"StreamlinerCore API","title":"StreamlinerCore.summarize","text":"summarize(io::IO, model::Model, data::AbstractData, training::Training)\n\nDisplay summary information concerning model (structure and number of parameters) and data (number of batches and size of each batch).\n\n\n\n\n\n","category":"function"},{"location":"nbody-guide/#N-Body-Problem-Guide","page":"N-Body Problem Guide","title":"N-Body Problem Guide","text":"","category":"section"},{"location":"nbody-guide/","page":"N-Body Problem Guide","title":"N-Body Problem Guide","text":"The data is prepared and saved as .csv files, representing trajectories with their key attributes.","category":"page"},{"location":"nbody-guide/","page":"N-Body Problem Guide","title":"N-Body Problem Guide","text":"We use two data representations:","category":"page"},{"location":"nbody-guide/","page":"N-Body Problem Guide","title":"N-Body Problem Guide","text":"Each orbit is seen a sequence of points in the (x,y,z)-space, so that each row is an instant photo of a trajectory in the space (orbits_db.csv);","category":"page"},{"location":"nbody-guide/","page":"N-Body Problem Guide","title":"N-Body Problem Guide","text":"(Image: )","category":"page"},{"location":"nbody-guide/","page":"N-Body Problem Guide","title":"N-Body Problem Guide","text":"Each orbit is an object which contains its time duration as a trajectory among its attributes (summary_orbits_db.csv). This is a compact representation of data, useful to predict qualitative attributes which do not involve the geometric shape of orbits.","category":"page"},{"location":"nbody-guide/","page":"N-Body Problem Guide","title":"N-Body Problem Guide","text":"(Image: )","category":"page"},{"location":"nbody-guide/","page":"N-Body Problem Guide","title":"N-Body Problem Guide","text":"Once the .csv files are created, it is enough to place them in the ./data folder in order to be processed in DashiBoard. Once the frontend is launched, it is enough to choose among the available files in the \"Load\" page","category":"page"},{"location":"nbody-guide/","page":"N-Body Problem Guide","title":"N-Body Problem Guide","text":"(Image: ) (Image: )","category":"page"},{"location":"nbody-guide/","page":"N-Body Problem Guide","title":"N-Body Problem Guide","text":"In the \"Filter\" page, it is possible to filter the loaded db. Filters range among the attributes of the Trajectory dataclass. Once the desired parameters are set, the \"Submit\" button provides the filtered db.","category":"page"},{"location":"nbody-guide/","page":"N-Body Problem Guide","title":"N-Body Problem Guide","text":"(Image: ) (Image: )","category":"page"},{"location":"nbody-guide/","page":"N-Body Problem Guide","title":"N-Body Problem Guide","text":"In the \"Process\" page it is possible to select one of the available cards which provide data analysis tools. For instance, the \"Split\" card returns partitions of the dataset with a prescribed order and grouping.","category":"page"},{"location":"nbody-guide/","page":"N-Body Problem Guide","title":"N-Body Problem Guide","text":"(Image: ) (Image: )","category":"page"},{"location":"#Overview-of-DashiBoard","page":"Overview","title":"Overview of DashiBoard","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"DashiBoard is a data visualization GUI written in the Julia programming language.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"The backend is powered by three libraries:","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"DuckDBUtils,\nDataIngestion,\nPipelines,\nStreamlinerCore.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"The frontend is powered by SolidJS.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"To see how to get started, proceed to the next section.","category":"page"},{"location":"dl-guide/#Deep-Learning-Guide","page":"Deep Learning Guide","title":"Deep Learning Guide","text":"","category":"section"},{"location":"dl-guide/","page":"Deep Learning Guide","title":"Deep Learning Guide","text":"DashiBoard supports training and evaluating deep learning models via the StreamlinerCore package.","category":"page"},{"location":"dl-guide/","page":"Deep Learning Guide","title":"Deep Learning Guide","text":"Streamliner cards can be inserted into any pipeline and receive / provide data to/from any other card.","category":"page"},{"location":"dl-guide/#Setup","page":"Deep Learning Guide","title":"Setup","text":"","category":"section"},{"location":"dl-guide/","page":"Deep Learning Guide","title":"Deep Learning Guide","text":"As designing a deep learning model and devising a training procedure are complex tasks, DashiBoard request the user to pass in folders with example configuration files (see the static/model and static/training folders for examples).","category":"page"},{"location":"dl-guide/","page":"Deep Learning Guide","title":"Deep Learning Guide","text":"When using a streamliner card, DashiBoard will read those folder and automatically generate the appropriate widgets.","category":"page"},{"location":"dl-guide/#Visualization","page":"Deep Learning Guide","title":"Visualization","text":"","category":"section"},{"location":"dl-guide/","page":"Deep Learning Guide","title":"Deep Learning Guide","text":"The streamliner card comes with a default visualization (see Pipelines.visualize), displaying the trajectory of the loss function in the training and validation datasets.","category":"page"},{"location":"dl-guide/#Example-pipeline","page":"Deep Learning Guide","title":"Example pipeline","text":"","category":"section"},{"location":"dl-guide/","page":"Deep Learning Guide","title":"Deep Learning Guide","text":"Below we see a typical deep learning pipeline, comprised of the following steps:","category":"page"},{"location":"dl-guide/","page":"Deep Learning Guide","title":"Deep Learning Guide","text":"data partition,\ndata normalization,\nmodel training and evaluation.","category":"page"},{"location":"dl-guide/","page":"Deep Learning Guide","title":"Deep Learning Guide","text":"(Image: deep learning pipeline)","category":"page"},{"location":"lib/DuckDBUtils/#DuckDBUtils","page":"DuckDBUtils API","title":"DuckDBUtils","text":"","category":"section"},{"location":"lib/DuckDBUtils/#Database-interface","page":"DuckDBUtils API","title":"Database interface","text":"","category":"section"},{"location":"lib/DuckDBUtils/#DuckDBUtils.Repository","page":"DuckDBUtils API","title":"DuckDBUtils.Repository","text":"Repository(db::DuckDB.DB)\n\nConstruct a Repository object that holds a DuckDB.DB as well as a pool of connections.\n\nUse DBInterface.(f::Base.Callable, repository::Repository, sql::AbstractString, [params]) to run a function on the result of a query sql on an available connection in the pool.\n\n\n\n\n\n","category":"type"},{"location":"lib/DuckDBUtils/#DuckDBUtils.get_catalog","page":"DuckDBUtils API","title":"DuckDBUtils.get_catalog","text":"get_catalog(repository::Repository; schema = nothing)\n\nExtract the catalog of available tables from a Repository repository.\n\n\n\n\n\n","category":"function"},{"location":"lib/DuckDBUtils/#DuckDBUtils.acquire_connection","page":"DuckDBUtils API","title":"DuckDBUtils.acquire_connection","text":"acquire_connection(repository::Repository)\n\nAcquire an open connection to the database repository.db from the pool repository.pool. See also release_connection.\n\nnote: Note\nA command con = acquire_connection(repository) must always be followed by a matching command release_connection(repository, con) (after the connection has been used).\n\n\n\n\n\n","category":"function"},{"location":"lib/DuckDBUtils/#DuckDBUtils.release_connection","page":"DuckDBUtils API","title":"DuckDBUtils.release_connection","text":"release_connection(repository::Repository, con)\n\nRelease connection con to the pool repository.pool\n\n\n\n\n\n","category":"function"},{"location":"lib/DuckDBUtils/#DuckDBUtils.with_connection","page":"DuckDBUtils API","title":"DuckDBUtils.with_connection","text":"with_connection(f, repository::Repository, [N])\n\nAcquire a connection con from the pool repository.pool. Then, execute f(con) and release the connection to the pool. An optional parameter N can be passed to determine the number of connections to be acquired (defaults to 1).\n\n\n\n\n\n","category":"function"},{"location":"lib/DuckDBUtils/#DuckDBUtils.render_params","page":"DuckDBUtils API","title":"DuckDBUtils.render_params","text":"render_params(catalog::SQLCatalog, node::SQLNode, params = (;))\n\nReturn query string and parameter list from query expressed as node.\n\n\n\n\n\n","category":"function"},{"location":"lib/DuckDBUtils/#DuckDBUtils.to_sql","page":"DuckDBUtils API","title":"DuckDBUtils.to_sql","text":"to_sql(x)\n\nConvert a julia value x to its SQL representation.\n\n\n\n\n\n","category":"function"},{"location":"lib/DuckDBUtils/#Table-tools","page":"DuckDBUtils API","title":"Table tools","text":"","category":"section"},{"location":"lib/DuckDBUtils/#DuckDBUtils.load_table","page":"DuckDBUtils API","title":"DuckDBUtils.load_table","text":"load_table(\n    repository::Repository,\n    table,\n    name::AbstractString;\n    schema = nothing\n)\n\nLoad a Julia table table as name in schema schema in repository.db.\n\n\n\n\n\n","category":"function"},{"location":"lib/DuckDBUtils/#DuckDBUtils.replace_table","page":"DuckDBUtils API","title":"DuckDBUtils.replace_table","text":"replace_table(\n    repository::Repository,\n    query::Union{AbstractString, SQLNode}\n    [params,]\n    name::AbstractString;\n    schema = nothing,\n    virtual::Bool = false\n)\n\nReplace table name in schema schema in repository.db with the result of a given query with optional parameters params.\n\nUse virtual = true to replace a view instead of a table.\n\n\n\n\n\n","category":"function"},{"location":"lib/DuckDBUtils/#DuckDBUtils.export_table","page":"DuckDBUtils API","title":"DuckDBUtils.export_table","text":"export_table(\n    repository::Repository,\n    query::Union{AbstractString, SQLNode}\n    [params,]\n    path::AbstractString;\n    schema = nothing,\n    options...\n)\n\nExport to path (with options options) the result of a given query with optional parameters params in schema schema in repository.db.\n\n\n\n\n\n","category":"function"},{"location":"lib/DuckDBUtils/#DuckDBUtils.delete_table","page":"DuckDBUtils API","title":"DuckDBUtils.delete_table","text":"delete_table(\n    repository::Repository,\n    name::AbstractString;\n    schema = nothing,\n    virtual::Bool = false\n)\n\nDelete table name in schema schema in repository.db.\n\nUse virtual = true to delete a view instead of a table.\n\n\n\n\n\n","category":"function"},{"location":"lib/DuckDBUtils/#DuckDBUtils.with_table","page":"DuckDBUtils API","title":"DuckDBUtils.with_table","text":"with_table(f, repository::Repository, table; schema = nothing)\n\nRegister a table under a random unique name name, apply f(name), and then unregister the table.\n\n\n\n\n\n","category":"function"},{"location":"lib/DuckDBUtils/#DuckDBUtils.colnames","page":"DuckDBUtils API","title":"DuckDBUtils.colnames","text":"colnames(repository::Repository, table::AbstractString; schema = nothing)\n\nReturn list of columns for a given table.\n\n\n\n\n\n","category":"function"},{"location":"lib/DuckDBUtils/#Batched-iteration","page":"DuckDBUtils API","title":"Batched iteration","text":"","category":"section"},{"location":"lib/DuckDBUtils/#DuckDBUtils.Batches","page":"DuckDBUtils API","title":"DuckDBUtils.Batches","text":"Batches(tbl, batchsize::Integer, nrows::Integer)\n\nConstruct a Batches iterator based on a table tbl with nrows in total. The resulting object iterates column-based tables with batchsize rows each.\n\n\n\n\n\n","category":"type"},{"location":"lib/DuckDBUtils/#Internal-functions","page":"DuckDBUtils API","title":"Internal functions","text":"","category":"section"},{"location":"lib/DuckDBUtils/#DuckDBUtils._numobs","page":"DuckDBUtils API","title":"DuckDBUtils._numobs","text":"_numobs(cols)\n\nCompute the number of rows of a column-based table cols.\n\n\n\n\n\n","category":"function"},{"location":"lib/DuckDBUtils/#DuckDBUtils._init","page":"DuckDBUtils API","title":"DuckDBUtils._init","text":"_init(schm::Tables.Schema)\n\nInitialize an empty table with schema schm.\n\n\n\n\n\n","category":"function"},{"location":"lib/DuckDBUtils/#DuckDBUtils._append!","page":"DuckDBUtils API","title":"DuckDBUtils._append!","text":"_append!(batch::AbstractDict, cols, rg = Colon())\n\nAppend rows rg of column-based table cols to the dict table batch.\n\n\n\n\n\n","category":"function"},{"location":"lib/DuckDBUtils/#DuckDBUtils.in_schema","page":"DuckDBUtils API","title":"DuckDBUtils.in_schema","text":"in_schema(name::AbstractString, schema::Union{AbstractString, Nothing})\n\nUtility to create a name to refer to a table within the schema.\n\nFor instance\n\njulia> print(in_schema(\"tbl\", nothing))\n\"tbl\"\njulia> print(in_schema(\"tbl\", \"schm\"))\n\"schm\".\"tbl\"\n\n\n\n\n\n","category":"function"}]
}
